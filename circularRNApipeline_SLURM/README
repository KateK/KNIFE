
** Before attempting to run, make sure you have the Bowtie2 index files for your species 
in circularRNApipeline_SLURM/index, the Bowtie1 index files in 
circularRNApipeline_SLURM/denovo_scripts/index, and the gtf file in 
circularRNApipeline_SLURM/denovo_scripts (see main project README for details).

** A complete analysis using these wrapper scripts to submit jobs to the scheduler requires 
calling findCircularRNA.sh 2 times. The first run generates the annotation-dependent reports, 
alignment statistics, and creates the de novo index for this dataset. For the second run, 
use all the same parameters from the first run but append the string "_unaligned" to the mode. 
This generates the de novo reports by aligning unaligned reads to the de novo index created 
for this dataset during the first run. 

** You must cd into circularRNApipeline_SLURM and run the script findCircularRNA.sh
from within the directory so that all relative paths will work.

** There are a total of 10 possible parameters, the first 5 are required and
defaults for the rest are described below. Keep in mind that since this is a
shell script the arguments are positional. This means that if you wanted to use
default value for argument 6 but wanted to specify a different value for
parameter 7, you still need to call the script with the default value for
argument 6 specified.

** see README on main project page for details on each of these parameters

##########################
## USAGE
###########################

sh findCircularRNA.sh read_directory read_id_style alignment_parent_directory
dataset_name junction_overlap mode report_directory_name ntrim denovoCircMode junction_id_suffix

** in addition to the possible values for the "mode" parameter described in the project README,
you can also append "_large" to the mode to request additional resources when submitting jobs
to the scheduler. If you find that some of your samples run fine but others do not, a good first 
step in troubleshooting is to rerun in the "large" mode (you can also confirm this by querying 
the scheduler to see how much memory and time your job ended up using).
